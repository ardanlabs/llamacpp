I need a Cobra CLI app (https://github.com/spf13/cobra) that provides the following
commands and sub-commands. Please just provide an empty functions for each of these
commands and sub-commands. Put the code in the main.go at this location
`cmd/server/main.go` and you can add any other files to `cmd/server/` that you
feel you need.

Commands:
```
Large language model runner

Usage:
  kronk [flags]
  kronk [command]

Available Commands:
  serve       Start kronk
  pull        Pull a model from a registry
  list        List models
  show        Show information for a model
  ps          List running models
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for kronk
  -v, --version   Show version information

Use "kronk [command] --help" for more information about a command.

---

$ kronk help serve
Start kronk

Usage
  kronk serve [flags]

Aliases:
  serve, start

Flags:
  -h, --help   help for serve

Environment Variables:
      KRONK_HOST                  (default: 127.0.0.1:11434) IP Address for the kronk server 
      KRONK_MODELS                (default: ./models)        The path to the models directory
      KRONK_LLM_LIBRARY           (default: ./libraries)     Set LLM library to bypass autodetection
      KRONK_DEVICE                (default: autodetection)   Device to use for inference 
      KRONK_MODEL_INSTANCES       (default: 1)               Maximum number of parallel requests
      KRONK_MODEL_CONTEXT_WINDOW  (default: 4096)            Context window to use for inference 
      KRONK_MODEL_NBatch          (default: 2048)            Logical batch size or the maximum number of tokens that can be in a single forward pass through the model at any given time
      KRONK_MODEL_NUBatch         (default: 512)             Physical batch size or the maximum number of tokens processed together during the initial prompt processing phase (also called "prompt ingestion") to populate the KV cache
      KRONK_MODEL_NThreads        (default: llama.cpp)       Number of threads to use for generation
      KRONK_MODEL_NThreadsBatch   (default: llama.cpp)       Number of threads to use for batch processing

---

$ kronk help pull
Pull a model from a registry

Usage:
  kronk pull MODEL_URL

Environment Variables:
      KRONK_HOST  IP Address for the kronk server (default 127.0.0.1:11434)

---

$ kronk help list
List models

Usage:
  kronk list

Environment Variables:
      KRONK_HOST  IP Address for the kronk server (default 127.0.0.1:11434)

---

$ kronk help show
Show information for a model

Usage:
  kronk show MODEL_NAME

Environment Variables:
      KRONK_HOST  IP Address for the kronk server (default 127.0.0.1:11434)

---

$ kronk help ps
List running models

Usage:
  kronk ps

Environment Variables:
      KRONK_HOST  IP Address for the kronk server (default 127.0.0.1:11434)

---

$ kronk help rm
Remove a model

Usage:
  kronk rm MODEL_NAME

Environment Variables:
      KRONK_HOST  IP Address for the kronk server (default 127.0.0.1:11434)
```